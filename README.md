
# Fine-tuning KantaiBERT: Training and Utilizing a Custom RoBERTa-Based Language Model for Masked Language Modeling

In this project, I undertook the task of fine-tuning a custom language model named KantaiBERT using the RoBERTa architecture. The model was initially trained on a corpus of philosophical texts by Immanuel Kant, downloaded from the [Gutenberg Project](https://www.gutenberg.org/). The objective was to enhance the model's ability to understand and generate coherent text by performing masked language modeling tasks. 

